{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Notebook for training and evaluating the dataset on a benchmark model DenseNet169",
   "id": "114d78f9dfb64f54"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9181eb4d-0da2-436f-8282-cc1dcacd217b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.62.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.25.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, markdown, tensorboard\n",
      "Successfully installed markdown-3.6 tensorboard-2.16.2 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de62ce7a-974e-4a1e-88fd-7b91733a08e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader, Subset, ConcatDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from google.cloud import storage\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb09081-3617-4b91-98a4-1e5e72c42a97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=420):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    # Python RNG\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # PyTorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    # Numpy RNG\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # OS RNG\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "def worker_init_fn(worker_id):    \n",
    "    \"\"\"Ensure that the data loading process is deterministic.\"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "set_seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bbf69a-a5d8-40e1-8f21-f0d5b2f39d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_id = 'deep-learning-420208'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f759612-011b-4d7c-8bed-5774b364961b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f480daf0-7b98-45ed-8987-74e2e4d3ef79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AugmentationTrafficSignLoader:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.augmentations = [folder for folder in os.listdir(root)\n",
    "                              if os.path.isdir(os.path.join(root, folder))]\n",
    "\n",
    "    def calculate_mean_and_variance(self, training_root, percentage_of_whole):\n",
    "    \n",
    "        transform = transforms.Compose([\n",
    "                transforms.Resize((96, 96)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "        test_dataset = datasets.ImageFolder(root=training_root, transform=transform)\n",
    "\n",
    "        indices = random.sample(population=list(range(len(test_dataset))), k=math.floor(len(test_dataset)*percentage_of_whole))\n",
    "        sample = Subset(test_dataset, indices)\n",
    "        loader = DataLoader(sample)\n",
    "\n",
    "        mean = 0.0\n",
    "        variance = 0.0\n",
    "        total_images = 0\n",
    "\n",
    "        for images, _ in loader:\n",
    "            # Rearrange batch to be the shape of [B, C, W * H]\n",
    "            images = images.view(images.size(0), images.size(1), -1)\n",
    "            # Update total_images\n",
    "            total_images += images.size(0)\n",
    "            # Compute mean and variance here\n",
    "            mean += images.mean(2).sum(0) \n",
    "            variance += images.var(2).sum(0)\n",
    "\n",
    "        # Final mean and variance\n",
    "        mean /= total_images\n",
    "        variance /= total_images\n",
    "\n",
    "        return mean, variance.sqrt()\n",
    "    \n",
    "    def augmentation_generator(self):\n",
    "        while self.augmentations:\n",
    "            current_aug = self.augmentations.pop()\n",
    "            current_aug_path = os.path.join(self.root, current_aug)\n",
    "            mean, std = self.calculate_mean_and_variance(current_aug_path, 0.25)\n",
    "            #print(mean, std)\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((96, 96)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=mean, std=std)\n",
    "            ])\n",
    "            yield os.path.basename(current_aug), datasets.ImageFolder(root=current_aug_path, transform=transform), mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb59d426-5423-4a72-9cba-67d5c58f7b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FineTuneDenseNet169(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FineTuneDenseNet169, self).__init__()\n",
    "        \n",
    "        # Load DenseNet169 with pretrained weights\n",
    "        self.densenet169 = models.densenet169(weights=models.DenseNet169_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove the last classifier layer (which is a Linear layer)\n",
    "        num_ftrs = self.densenet169.classifier.in_features\n",
    "        \n",
    "        # Add new classifier layers\n",
    "        self.densenet169.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 256),  # First new fully connected layer\n",
    "            nn.ReLU(inplace=True),     # Activation layer\n",
    "            nn.Linear(256, num_classes) # Second new fully connected layer mapping to the 33 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward through the modified DenseNet169 model\n",
    "        x = self.densenet169(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5930e56-cc14-403a-91a4-8d201bc51bc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_directory_to_gcs(bucket_name, source_directory, destination_blob_prefix):\n",
    "    \"\"\"Uploads a local directory and its subdirectories to a GCS bucket.\"\"\"\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(source_directory):\n",
    "        for filename in filenames:\n",
    "            local_file_path = os.path.join(dirpath, filename)\n",
    "            relative_path = os.path.relpath(local_file_path, source_directory)\n",
    "            blob_path = os.path.join(destination_blob_prefix, relative_path)\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(local_file_path)\n",
    "            #print(f\"File {local_file_path} uploaded to {blob_path}.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "benchmarks = [\n",
    "    ('FineTuneDenseNet169', FineTuneDenseNet169), \n",
    "]\n",
    "\n",
    "for bm_name, bm in benchmarks:\n",
    "    \n",
    "    TRAINING_DATA_ROOT, TEST_DATA_ROOT = 'benchmark-reduced/synthetic', 'benchmark-reduced/test_data'\n",
    "    \n",
    "    def calculate_accuracy(model, data_loader, device):\n",
    "        \"\"\"Calculates accuracy on given dataset\"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in data_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def evaluate_and_save_results(model, loader, device, classes, bucket_name, prefix, aug_name):\n",
    "        \"\"\"Evaluates the model and saves results and the model itself to Google Cloud Storage under a specific subfolder.\"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Compute the confusion matrix\n",
    "        cm = confusion_matrix(all_labels, all_preds, labels=np.arange(len(classes)))\n",
    "\n",
    "        # Folder prefix including augmentation name\n",
    "        full_prefix = f\"{prefix}/results/{aug_name}\"\n",
    "        os.makedirs(full_prefix)\n",
    "\n",
    "        # Save confusion matrix\n",
    "        cm_path = f\"{full_prefix}/confusion_matrix_{aug_name}.npy\"\n",
    "        np.save(cm_path, cm)\n",
    "\n",
    "        # Save predictions\n",
    "        preds_path = f\"{full_prefix}/predictions_{aug_name}.npy\"\n",
    "        np.save(preds_path, np.array(all_preds))\n",
    "\n",
    "        # Save the model\n",
    "        model_path = f\"{full_prefix}/model_{aug_name}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        return cm\n",
    "\n",
    "    def train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, device, writer, prefix, epochs=5, patience=20, min_delta=0.001):\n",
    "        best_val_loss = float('inf')\n",
    "        best_val_acc = 0\n",
    "        epochs_no_improve = 0\n",
    "        early_stop = False\n",
    "        # Path to save the best model\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(prefix)\n",
    "        best_model_path = f'{prefix}/best_model.pth'\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "\n",
    "            train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "            val_accuracy = calculate_accuracy(model, val_loader, device)\n",
    "            test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "\n",
    "            writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss}, epoch)\n",
    "            writer.add_scalars('Accuracy', {'Train': train_accuracy, 'Validation': val_accuracy}, epoch)\n",
    "\n",
    "            if val_loss < best_val_loss - min_delta:\n",
    "                #best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), best_model_path)  # Save the best model checkpoint\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                    early_stop = True\n",
    "                    break\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}%')\n",
    "\n",
    "        model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "        print(test_accuracy)\n",
    "        writer.add_scalar('Accuracy/test', test_accuracy, 1)\n",
    "        writer.close()\n",
    "        return test_accuracy\n",
    "\n",
    "\n",
    "    BUCKET_NAME = 'sign-recognition-metrics'\n",
    "    PREFIX = f'metrics_{bm_name}/' + datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "    train_augmentation_generator = AugmentationTrafficSignLoader(root=TRAINING_DATA_ROOT).augmentation_generator()\n",
    "    for aug_name, aug_variant, mean, std in train_augmentation_generator:\n",
    "        # Ensure GPU memory is clean before starting the setup\n",
    "        torch.cuda.empty_cache()\n",
    "        print(aug_name)\n",
    "\n",
    "        # Data loading setup\n",
    "        num_classes = len(aug_variant.classes)\n",
    "\n",
    "        #Model initialization and setup\n",
    "        model = bm(num_classes)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "        #print(aug_variant.classes)\n",
    "        train_size = int(0.8 * len(aug_variant))\n",
    "        val_size = len(aug_variant) - train_size\n",
    "        train_dataset, val_dataset = random_split(aug_variant, [train_size, val_size])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=16, worker_init_fn=worker_init_fn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=16, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        # Load the test dataset\n",
    "        transform = transforms.Compose([\n",
    "                    transforms.Resize((96, 96)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=mean, std=std)\n",
    "                ])\n",
    "        test_dataset = datasets.ImageFolder(root=TEST_DATA_ROOT, transform=transform)\n",
    "        idx_to_class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=16, worker_init_fn=worker_init_fn)\n",
    "\n",
    "        # TensorBoard Setup\n",
    "        writer_dir = os.path.join(PREFIX, 'runs', aug_name)\n",
    "        writer = SummaryWriter(writer_dir)\n",
    "\n",
    "        model_dir = f\"{PREFIX}/models/{aug_name}\"\n",
    "        # Training and Evaluation\n",
    "        test_accuracy = train_and_evaluate(model, train_loader, val_loader, test_loader, criterion, optimizer, device, writer, model_dir, epochs=50)\n",
    "        cm = evaluate_and_save_results(model, test_loader, device, idx_to_class, BUCKET_NAME, PREFIX, aug_name)\n",
    "\n",
    "        # Upload results\n",
    "        upload_directory_to_gcs(BUCKET_NAME, PREFIX, PREFIX)\n",
    "\n",
    "        # Clear memory\n",
    "        torch.cuda.empty_cache()"
   ],
   "id": "7e1d64afe0f8598c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0 (Local)",
   "language": "python",
   "name": "pytorch-2-0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
